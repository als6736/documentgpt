from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.callbacks.base import BaseCallbackHandler
import streamlit as st

st.set_page_config(
    page_title="DocumentGPT",
    page_icon="ğŸ“ƒ",
)


class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):# llmì´ ì–¸ì œ tokenì„ ìƒì„±í•˜ëŠ”ì§€
        self.message_box = st.empty()#llmì´ tokenì„ ìƒì„±í•˜ê¸° ì‹œì‘í•˜ë©´ í™”ë©´ì— empty boxë¥¼ ìƒì„±

    def on_llm_end(self, *args, **kwargs):# llmì´ ì–¸ì œ ì‘ì—…ì„ ëë‚´ëŠ”ì§€
        save_message(self.message, "ai")# ì‘ì—…ì™„ë£Œì‹œ ë©”ì‹œì§€ ì €ì¥

    def on_llm_new_token(self, token, *args, **kwargs):# llmì´ ì–¸ì œ new tokenì„ ìƒì„±í•˜ëŠ”ì§€
        self.message += token# ë©”ì‹œì§€ì— í•˜ë‚˜ì”© í† í°ë“¤ì„ ì‹¤ì‹œê°„ì„ ì¶”ê°€
        self.message_box.markdown(self.message) #ì´í›„ ì—…ë°ì´íŠ¸


llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)


@st.cache_data(show_spinner="Embedding file...")
#computationì„ cache. ì§ˆë¬¸ì„ ì…ë ¥í• ë•Œë§ˆë‹¤ íŒŒì¼ì„ ë‹¤ì‹œ ì½ì§€ ì•Šê¸°í•˜ê¸°ìœ„í•´
#ì…ë ¥ì´ ë³€ê²½ë˜ì§€ ì•ŠëŠ”í•œ, ë™ì¼í•œ íŒŒì¼ì„ í•¨ìˆ˜ì— ê³„ì† ë³´ë‚´ë©´ í•¨ìˆ˜ê°€ ë‹¤ì‹œ ì‹¤í–‰ë˜ì§€ ì•ŠëŠ”ë‹¤
def embed_file(file): #íŒŒì¼ ì„ë² ë”©
    file_content = file.read()#íŒŒì¼ì„ ì½ê³  ë³µì‚¬í•˜ê³ 
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    #splitterë¥¼ ìƒì„±
    loader = UnstructuredFileLoader(file_path)
    #ë‹¤ì–‘í•œ íŒŒì¼ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    docs = loader.load_and_split(text_splitter=splitter)
    #ì—¬ëŸ¬ê°œì˜ documentë¡œ ë‚˜ëˆ  ì„±ëŠ¥ì„ ë†’ì´ê³  apiì´ìš©ì— ë“œëŠ” ë¹„ìš©ì„ ë”ìš± ì¤„ì„
    embeddings = OpenAIEmbeddings()
    #openai ì„ë² ë”©ëª¨ë¸ ì‚¬ìš©
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)
    # CacheBackedEmbeddingsë¡œë§Œë“¤ì–´ì§„ embeddingì„ cacheë¡œ ì €ì¥
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    #docs, cached_embeddings í•¨ê»˜ from_documentsë©”ì„œë“œ í˜¸ì¶œ documentë³„ë¡œ embeddingì‘ì—…í›„ ê²°ê³¼ë¥¼ ì €ì¥í•œ vector storeë¥¼ ë°˜í™˜ ì´ë¥¼ ì´ìš©í•˜ì—¬ document ê²€ìƒ‰ë° ì—°ê´€ì„±ì´ ë†’ì€ documentë“¤ì„ ì°¾ê¸°ë„í•¨
    retriever = vectorstore.as_retriever()
    return retriever# retrieverë¡œ ë³€ê²½ chainì—ì„œ ì‚¬ìš©í• ìˆ˜ ìˆë„ë¡
    #retrieverëŠ” documentsë¥¼ ì œê³µí•˜ëŠ”ê²ƒ


def save_message(message, role): #message ë”•ì…”ë„ˆë¦¬ ë”í•˜ëŠ” function
    st.session_state["messages"].append({"message": message, "role": role})


def send_message(message, role, save=True): #message send function
    with st.chat_message(role):# roleì—ëŠ” AIë‚˜ humanì´ ì˜¬ìˆ˜ ìˆê³ ,
        st.markdown(message)
    if save:
        save_message(message, role)# ì„ íƒì ìœ¼ë¡œ messageë¥¼ cacheì— saveí• ìˆ˜ ìˆìŒ


def paint_history():# message storeì˜ ëª¨ë“  ë©”ì‹œì§€ì— ëŒ€í•´ì„œ send messageë¥¼ í†µí•´ í™”ë©´ì— ë©”ì‹œì§€ ì¶œë ¥
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False, #ê·¸ ë©”ì‹œì§€ë¥¼ ì €ì¥í•˜ì§€ëŠ” ì•ŠìŒ, ì´ë¯¸ ì €ì¥ëœ ê²ƒì´ê¸° ë•Œë¬¸ì—
        )


def format_docs(docs):#\n ì¤„ë°”ê¿ˆ ë¬¸ìë¡œ êµ¬ë¶„ëœ í•˜ë‚˜ì˜ stringìœ¼ë¡œ í•©ì³ì§
    return "\n\n".join(document.page_content for document in docs)


prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Answer the question using ONLY the following context. If you don't know the answer just say you don't know. DON'T make anything up.
            
            Context: {context}
            """,
        ),
        ("human", "{question}"),
    ]
)


st.title("DocumentGPT")

st.markdown(
    """
Welcome!
            
Use this chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)

with st.sidebar:#ìŠ¤íŠ¸ë¦¼ë¦¿ì„ ì‚¬ìš©í•œ ì‚¬ì´ë“œë°”
    file = st.file_uploader(#íŒŒì¼ ì—…ë¡œë“œ ìš”ì²­, ì²˜ìŒì—ëŠ” íŒŒì¼ì´ ì—†ìŒ
        "Upload a .txt .pdf or .docx file",
        type=["pdf", "txt", "docx"],
    )

if file: #ìœ ì €ê°€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ë©´ ê·¸ íŒŒì¼ë¡œ ë¶€í„° retrieverë¥¼ ì–»ëŠ”ë‹¤
    retriever = embed_file(file)
    #íŒŒì¼ì„ ì„ë² ë”©í•œ ë’¤ì— send message functionì„ ì‹¤í–‰í•œë‹¤
    send_message("I'm ready! Ask away!", "ai", save=False)
    #send message functionì€ ë©”ì‹œì§€ë¥¼ í™”ë©´ì— í‘œì‹œí•˜ê³  ìºì‹œì— ì €ì¥
    paint_history()
    message = st.chat_input("Ask anything about your file...")
    if message:# ì‚¬ìš©ìê°€ ë©”ì‹œì§€ë¥¼ ë³´ë‚¼ì‹œ humanìœ¼ë¡œ ë³´ëƒ„
        send_message(message, "human")
        chain = (
            {   #retieverëŠ” documentì˜ list ì œê³µ
                "context": retriever | RunnableLambda(format_docs),
                #document listë¥¼  format_docsì— ì œê³µ
                "question": RunnablePassthrough(),
                #ì²´ì¸ì„ invoke í•˜ê³  ì´ê³³ì— ì–´ë–¤ ì§ˆë¬¸ì„í•˜ë©´ ìˆ˜ì •ë˜ì§€ ì•Šê³  promptë¡œ ì´ë™ë˜ê¸¸ ë°”ëŒ
            }#ì—¬ê¸°ê¹Œì§€ê°€ promptì—ëŒ€í•œ input
            | prompt#ëŠ” doumentì™€ ì§ˆë¬¸ì„ ì„¤ì •í•˜ê³  ê·¸ê²ƒì„ llmëª¨ë¸ë¡œ ë³´ëƒ„
            | llm
        )
        with st.chat_message("ai"):
            chain.invoke(message)


else: 
    st.session_state["messages"] = []
    #session stateì— ìˆëŠ” ë©”ì‹œì§€ ì €ì¥ì†Œë¥¼ empty listë¡œ ì´ˆê¸°í™”